{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Some problems\n",
    "#### remove morph function from Network class to shift to hill climbing, also kernel_size and widening factor \n",
    "#### for deepen and widen should be parameters\n",
    "### check more conformity betweeen input adj_list and input 'int_to_node'\n",
    "### check compatibility of initial arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from graphviz import Digraph\n",
    "import random\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10a249d90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Error(Exception):\n",
    "    def __init__(self, expr = None, msg = None):\n",
    "        self.expr = expr\n",
    "        self.msg = msg\n",
    "class inputSmallerThanKernel(Error):\n",
    "    def __init__(self):\n",
    "        super(inputSmallerThanKernel, self).__init__()\n",
    "class nodeDoesNotExist(Error):\n",
    "    def __init__(self):\n",
    "        super(nodeDoesNotExist, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node(object):\n",
    "    nodes = []\n",
    "    node_type = 'simple'\n",
    "    def __init__(self, input_shape = (0, 0, 0), output_shape = (0, 0, 0)): # c, i1, i2\n",
    "        node.nodes.append(self)\n",
    "        self.no  = len(node.nodes)\n",
    "        self.in_adj = []\n",
    "        self.out_adj = []\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.compatible = True\n",
    "            \n",
    "    def node_alright(self, curr_node):\n",
    "        try:\n",
    "            assert(issubclass(type(curr_node), node))\n",
    "        except:\n",
    "            raise Error('Not a node' + str(type(curr_node)))\n",
    "# Put this section in graph class\n",
    "#         try:\n",
    "#             assert(curr_node in graph_nodes)\n",
    "#         except:\n",
    "#             raise nodeDoesNotExist\n",
    "    \n",
    "    def determine_compatibility(self):\n",
    "        for curr_node in self.in_adj:\n",
    "            curr = (curr_node.output_shape == self.input_shape)\n",
    "            self.compatible  = self.compatible and curr\n",
    "            \n",
    "        for curr_node in self.out_adj:\n",
    "            curr = (curr_node.input_shape == self.output_shape)\n",
    "            self.compatible = self.compatible and curr\n",
    "\n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        assert isinstance(in_adj, list), 'in_adj must be a list'\n",
    "        assert isinstance(in_adj, list), 'out_adj must be a list'\n",
    "        for curr_node in in_adj + out_adj:\n",
    "            try:\n",
    "                self.node_alright(curr_node)\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "        self.in_adj = in_adj\n",
    "        self.out_adj = out_adj\n",
    "\n",
    "    def out_shape(self):\n",
    "        pass\n",
    "    \n",
    "    def remove(self):\n",
    "        ## needed in graph class\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.no) + \" \" + str(node.node_type) \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution_block(nn.Module, node):\n",
    "    all_convs = []\n",
    "    node_type = 'conv'\n",
    "    def __init__(self, in_h, in_w, in_channels, out_channels, kernel_size, padding = 0, stride = 1):\n",
    "        try:\n",
    "            assert(min(in_h, in_w) +2*padding >= kernel_size)\n",
    "        except:\n",
    "            raise inputSmallerThanKernel\n",
    "        super(convolution_block, self).__init__()\n",
    "        node.__init__(self, (in_channels, in_h, in_w))\n",
    "        convolution_block.all_convs.append(self)\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride \n",
    "        self.padding = padding\n",
    "        self.output_shape = self.out_shape()\n",
    "        \n",
    "        # NN Layers\n",
    "        self.conv_layer = nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
    "        self.batch_norm = nn.BatchNorm2d(self.out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    def out_shape(self):\n",
    "        c, h, w = self.input_shape\n",
    "        C = self.out_channels\n",
    "        H = (h + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        W = (w + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        return (C, H, W)\n",
    "    \n",
    "#     def determine_compatibility(self):\n",
    "#         super(convolution_block, self).determine_compatibility()\n",
    "#         self.compatible  = self.compatible and (len(self.in_adj) == 1)\n",
    "\n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        super(convolution_block, self).describe_adj_list(in_adj, out_adj)\n",
    "        try:\n",
    "            assert(len(in_adj) == 1)\n",
    "        except:\n",
    "#             print(in_adj)\n",
    "            raise Error('A convolution block can have only one in-edge')\n",
    "    \n",
    "    def remove(self): \n",
    "        ### Remove from all_convs list\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.no) + \" \" + str(convolution_block.node_type) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pool_node(nn.Module, node):\n",
    "    all_max_pools = []\n",
    "    node_type = 'max_pool'    \n",
    "    def __init__(self, in_h, in_w, in_channels, kernel_size, padding = 0, stride = 1):\n",
    "        try:\n",
    "            assert(min(in_h, in_w) +2*padding > kernel_size)\n",
    "        except:\n",
    "            raise inputSmallerThanKernel\n",
    "        super(max_pool_node, self).__init__()    \n",
    "        node.__init__(self, (in_channels, in_h, in_w))\n",
    "        max_pool_node.all_max_pools.append(self)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride \n",
    "        self.padding = padding\n",
    "        self.output_shape = self.out_shape()\n",
    "        \n",
    "        ## NN Layer\n",
    "        self.max_pool_layer = nn.MaxPool2d(self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.max_pool_layer(x)\n",
    "    \n",
    "    def out_shape(self):\n",
    "        c, h, w = self.input_shape\n",
    "        C = c\n",
    "        H = (h + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        W = (w + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        return (C, H, W)\n",
    "    \n",
    "#     def determine_compatibility(self):\n",
    "#         super(max_pool_node, self).determine_compatibility()\n",
    "#         self.compatible  = self.compatible and (len(self.in_adj) == 1)\n",
    "\n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        super(max_pool_node, self).describe_adj_list(in_adj, out_adj)\n",
    "        try:\n",
    "            assert(len(in_adj) == 1)\n",
    "        except:\n",
    "            raise Error('A max-pool block can have only one in-edge')\n",
    "\n",
    "    def remove(self):\n",
    "        pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.no) + \" \" + str(max_pool_node.node_type) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Is is it required to be to a derived class of nn.Module ?\n",
    "class merge_node(node):\n",
    "    all_merge_nodes = []\n",
    "    node_type = 'merge'    \n",
    "    def __init__(self, parents, child):\n",
    "        super(merge_node, self).__init__()\n",
    "#         node.__init__(self)\n",
    "        try:\n",
    "            self.describe_adj_list(parents, child)\n",
    "        except Exception as e:\n",
    "            print e.expr\n",
    "            raise e\n",
    "        merge_node.all_merge_nodes.append(self)\n",
    "        \n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        super(merge_node, self).describe_adj_list(in_adj, out_adj)\n",
    "        try:\n",
    "            assert(len(in_adj) == 2)\n",
    "        except:\n",
    "            raise Error('Parents must be exactly two')\n",
    "        \n",
    "class add_node(nn.Module, merge_node):\n",
    "    all_add_nodes = []\n",
    "    node_type = 'add'    \n",
    "    def __init__(self, parents, child):\n",
    "        super(add_node, self).__init__()\n",
    "        merge_node.__init__(self, parents, child)\n",
    "        add_node.all_add_nodes.append(self)\n",
    "        self.input_shape = self.in_adj[0].output_shape\n",
    "        self.output_shape = self.out_shape()\n",
    "                \n",
    "    ### Does it allow to input paramteters ?\n",
    "    def forward(self, x, y):\n",
    "        return x+y \n",
    "        \n",
    "    def out_shape(self):\n",
    "        return self.input_shape\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.no) + \" \" + str(add_node.node_type) \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "class concat_node(merge_node):\n",
    "    # Define Later\n",
    "    pass\n",
    "\n",
    "class convex_merge_node(merge_node):\n",
    "    # Define Later\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fullyConnected(nn.Module,node) :\n",
    "    def __init__(self, net) :   #nas\n",
    "        super(fullyConnected,self).__init__()\n",
    "        node.__init__(self, )\n",
    "        self.net = net\n",
    "        sh = net.int_to_node[net.topsort[-1]].output_shape\n",
    "        out = sh[0]*sh[1]*sh[2]\n",
    "#         out = nasout[1]*nasout[2]*nasout[3]\n",
    "        self.fc1 = nn.Linear(out,16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16,10)\n",
    "#         self.batch_size = nasout[0]\n",
    "        \n",
    "    def forward(self,x) :\n",
    "        out = self.net(x)\n",
    "#         out = out.view(self.batch_size,-1)\n",
    "#         print(out.shape)\n",
    "        out  =  out.view(out.shape[0], -1)\n",
    "#         print 'out.shape', out.shape\n",
    "#         [1 x 75264], m2: [2352 x 16]\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "BATCH = 4\n",
    "BEGIN_IN_CHANNELS = 3 \n",
    "def addLinearLayers(net):\n",
    "#     t = [32,3,28,28]\n",
    "    net = fullyConnected(net)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module, object):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.adj_mat = {}\n",
    "        self.adj_list = {}\n",
    "        self.nodes = []\n",
    "        self.int_to_node = {}\n",
    "        self.node_to_int = {}\n",
    "        self.conv_blocks = []\n",
    "        self.max_pool_blocks = [] # Change naming conv maybe ?\n",
    "        self.topsort = []\n",
    "        self.rank_in_topsort = {}\n",
    "        self.max_no = 0\n",
    "        self.node_dict = {}\n",
    "        \n",
    "        #self.fc = nn.Linear(3 * 28 * 28, 10)\n",
    "        \n",
    "    def __init__(self, adj_list, int_to_node):\n",
    "        super(Network, self).__init__()\n",
    "        assert isinstance(int_to_node, dict), 'int_to_node must be a dictionary'\n",
    "        for _, cnode in int_to_node.items():\n",
    "            assert isinstance(cnode, node), 'mapping in int_to_node should be to a node'\n",
    "        \n",
    "        assert isinstance(adj_list, dict), 'adj_list should be a dictionary'\n",
    "        assert(len(int_to_node) == len(adj_list))\n",
    "        for cnode, li in adj_list.items():\n",
    "            assert cnode in int_to_node, 'mismatch between int_to_node and adj_list'\n",
    "            try:\n",
    "                assert(isinstance(li, list))\n",
    "                assert(len(li) == 2)\n",
    "                assert(isinstance(li[0], list) and isinstance(li[1], list))\n",
    "            except:\n",
    "                raise Error('Each mapping in adj_list should be to a two-dim list')\n",
    "            for child_node in li[0]:\n",
    "                assert child_node in int_to_node, 'mismatch between int_to_node and adj_list'\n",
    "            for child_node in li[1]:\n",
    "                assert child_node in int_to_node, 'mismatch between int_to_node and adj_list'\n",
    "\n",
    "        self.adj_list = adj_list\n",
    "        self.adj_mat = self.get_adj_mat(self.adj_list)\n",
    "        self.nodes = int_to_node.keys()\n",
    "        self.int_to_node = int_to_node\n",
    "        self.node_dict = {}\n",
    "        self.node_to_int = self.get_node_to_int(self.int_to_node)\n",
    "        self.max_no = max(self.int_to_node)\n",
    "        self.conv_blocks, self.max_pool_blocks = self.get_conv_and_max_pool_blocks()\n",
    "        self.topsort = []\n",
    "        self.rank_in_topsort = {}\n",
    "        self.topsorting()\n",
    "        # hardcoded part\n",
    "        #self.fc = torch.nn.Linear(3 * 28 * 28, 10)\n",
    "        \n",
    "        \n",
    "    def createModel(self):\n",
    "         self.node_dict = torch.nn.ModuleDict(self.node_dict)\n",
    "         #print(self.node_dict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.topsorting() # though this is not required here\n",
    "        outputs = {}\n",
    "#         outputs[self.topsort[0]] = self.int_to_node[self.topsort[0]].forward(x)\n",
    "#       This is a bit inconsistent with the design patt\n",
    "        \n",
    "       # print 'self.topsort ',self.topsort\n",
    "        \n",
    "        outputs[self.topsort[0]] = x # instead add identity forward function to node class for dummy object\n",
    "        for ind in range(1, len(self.topsort)):\n",
    "            node_no = self.topsort[ind]\n",
    "            curr_node = self.int_to_node[node_no]\n",
    "            outputs[node_no] = curr_node.forward(*map(lambda x: outputs[x], self.adj_list[node_no][0]))\n",
    "        return outputs[self.topsort[-1]]\n",
    "    \n",
    "    def get_node_to_int(self, int_to_node):\n",
    "        node_to_int = {}\n",
    "        for no, cnode in int_to_node.items():\n",
    "            node_to_int[cnode] = no\n",
    "        return node_to_int\n",
    "\n",
    "        \n",
    "    def get_adj_mat(self, adj_list):\n",
    "        adj_mat = {}\n",
    "        nodes = adj_list.keys()\n",
    "        for x in nodes:\n",
    "            adj_mat[x] = {}\n",
    "            for y in nodes:\n",
    "                adj_mat[x][y] = 0\n",
    "        for cnode, li in adj_list.items():\n",
    "            for par in li[0]:\n",
    "                adj_mat[par][cnode] = 1\n",
    "            for child in li[1]:\n",
    "                adj_mat[cnode][child] = 1\n",
    "        return adj_mat\n",
    "    \n",
    "    def get_conv_and_max_pool_blocks(self):\n",
    "        conv_blocks = []\n",
    "        max_pool_blocks = []\n",
    "        for x in self.nodes:\n",
    "            if isinstance(self.int_to_node[x], convolution_block):\n",
    "                conv_blocks.append(x)\n",
    "                self.node_dict[str(x)]=self.int_to_node[x]\n",
    "            elif isinstance(self.int_to_node[x], max_pool_node):\n",
    "                max_pool_blocks.append(x)\n",
    "                self.node_dict[str(x)]=self.int_to_node[x]\n",
    "        return (conv_blocks, max_pool_blocks)\n",
    "    \n",
    "    def topsorting(self):\n",
    "        # level problem\n",
    "        topsort = []\n",
    "        import Queue\n",
    "        in_deg = {}\n",
    "        q = Queue.Queue()\n",
    "        for node in self.nodes:\n",
    "            val  = len(self.adj_list[node][0])\n",
    "#             val = len(self.int_to_node[node].in_adj)\n",
    "            if val == 0:\n",
    "                q.put(node)\n",
    "            in_deg[node] = val\n",
    "            \n",
    "        while not q.empty():\n",
    "            curr_node = q.get()\n",
    "            topsort.append(curr_node)\n",
    "            for child in self.adj_list[curr_node][1]:\n",
    "                in_deg[child] -= 1\n",
    "                if in_deg[child] == 0:\n",
    "                    q.put(child)\n",
    "        self.topsort = topsort\n",
    "        self.set_rank_in_topsort()\n",
    "    \n",
    "    def set_rank_in_topsort(self):\n",
    "        for ind, node_no in enumerate(self.topsort):\n",
    "            self.rank_in_topsort[node_no]  = ind\n",
    "    \n",
    "    def add_nodes_to_network(self, nodes):\n",
    "        ### loophole here, assumption is that all changed nodes are being provided to the function\n",
    "        ### for now, lets go on with it, but its an issue\n",
    "        for curr_node in nodes:\n",
    "            curr_node.determine_compatibility()\n",
    "            if not curr_node.compatible:\n",
    "                raise Error('Node is not compatible with the graph') \n",
    "        for curr_node in nodes:\n",
    "            if curr_node not in self.node_to_int:\n",
    "                self.max_no += 1\n",
    "                self.adj_mat[self.max_no] = {}\n",
    "                self.adj_list[self.max_no] = [[], []]\n",
    "                self.node_to_int[curr_node] = self.max_no\n",
    "                self.int_to_node[self.max_no] = curr_node\n",
    "                self.nodes.append(self.max_no)\n",
    "                if isinstance(curr_node, convolution_block):\n",
    "                    self.conv_blocks.append(self.max_no)\n",
    "                    self.node_dict[str(self.max_no)]=curr_node\n",
    "                    \n",
    "                elif isinstance(curr_node, max_pool_node):\n",
    "                    self.max_pool_blocks.append(self.max_no)\n",
    "                    self.node_dict[str(self.max_no)]=curr_node\n",
    "        for curr_node in nodes:\n",
    "            no = self.node_to_int[curr_node]\n",
    "            try:\n",
    "                self.adj_list[no] = [map(lambda x: self.node_to_int[x], curr_node.in_adj), map(lambda x: self.node_to_int[x], curr_node.out_adj)]\n",
    "            except:\n",
    "#                 print self.node_to_int\n",
    "#                 print curr_node.in_adj\n",
    "#                 print curr_node.out_adj\n",
    "                raise Exception\n",
    "            for par in self.adj_list[no][0]:\n",
    "                self.adj_mat[par][no] = 1\n",
    "            for child in self.adj_list[no][0]:\n",
    "                self.adj_mat[no][child] = 1\n",
    "        self.topsorting()\n",
    "            \n",
    "    def deepen_morph(self):\n",
    "        deepen_conv_block = self.int_to_node[random.choice(self.conv_blocks)]\n",
    "        kernel_size = random.choice([3, 5])\n",
    "        in_channels, in_h, in_w = deepen_conv_block.output_shape\n",
    "        out_channels = in_channels\n",
    "        identity_conv_block = convolution_block(in_h, in_w, in_channels, out_channels, kernel_size, (kernel_size-1)/2)\n",
    "        weights = identity_conv_block.conv_layer.weight.data\n",
    "        \n",
    "        # creating identity weights\n",
    "        for channel in range(out_channels):\n",
    "            for i in range(in_channels):\n",
    "                for j in range(kernel_size):\n",
    "                    for k in range(kernel_size):\n",
    "                        weights[channel][i][j][k] = int((channel == i) and (j == k) and j == (kernel_size)/2 )\n",
    "#         print 'weights of identity conv block', weights\n",
    "        \n",
    "        ## make connections \n",
    "        identity_conv_block.describe_adj_list([deepen_conv_block], deepen_conv_block.out_adj)\n",
    "        deepen_conv_block.describe_adj_list(deepen_conv_block.in_adj, [identity_conv_block])\n",
    "\n",
    "        #### later look at creating a function for singular change to in_adj or out_adj of nodes\n",
    "        for out_node in identity_conv_block.out_adj:\n",
    "            out_node_in_adj = [identity_conv_block if (x == deepen_conv_block) else x for x in out_node.in_adj ]\n",
    "            out_node.describe_adj_list(out_node_in_adj, out_node.out_adj)\n",
    "        \n",
    "        self.add_nodes_to_network([deepen_conv_block, identity_conv_block] + identity_conv_block.out_adj)\n",
    "    \n",
    "    \n",
    "    def widen_morph(self):\n",
    "        candidate_conv_blocks = []\n",
    "        for conv_block in self.conv_blocks:\n",
    "            isCandidate = bool(len(self.adj_list[conv_block][1]))\n",
    "            for child in self.adj_list[conv_block][1]:\n",
    "                isCandidate = isCandidate and isinstance(self.int_to_node[child], convolution_block)\n",
    "            if isCandidate:\n",
    "                candidate_conv_blocks.append(conv_block)\n",
    "        if len(candidate_conv_blocks) == 0:\n",
    "            return False\n",
    "\n",
    "        parent_block_no = random.choice(candidate_conv_blocks)\n",
    "        parent_block = self.int_to_node[parent_block_no]\n",
    "        widening_factor = random.choice([2, 4])\n",
    "        in_channels, in_h, in_w = parent_block.input_shape\n",
    "        out_channels = parent_block.out_channels\n",
    "        kernel_size = parent_block.kernel_size\n",
    "        padding = parent_block.padding\n",
    "        stride = parent_block.stride\n",
    "        widened_parent_block = convolution_block(in_h, in_w, in_channels, out_channels*widening_factor, kernel_size, padding, stride)\n",
    "        original_parent_weight = parent_block.conv_layer.weight.data\n",
    "        widened_parent_weight = widened_parent_block.conv_layer.weight.data\n",
    "        widened_parent_weight[:out_channels] = original_parent_weight\n",
    "        widened_parent_weight[out_channels:] = torch.zeros((out_channels*(widening_factor-1), in_channels, kernel_size, kernel_size))\n",
    "        parent_block = widened_parent_block\n",
    "#         self.int_to_node[parent_block_no]  = widened_parent_block\n",
    "#         del self.node_to_int[parent_block]\n",
    "#         self.node_to_int[widened_parent_block] = parent_block_no\n",
    "        parent_out_adj = []\n",
    "        for child in parent_block.out_adj:\n",
    "            child_no = self.node_to_int[child]\n",
    "            in_channels, in_h, in_w = child.input_shape\n",
    "            out_channels = child.out_channels\n",
    "            kernel_size = child.kernel_size \n",
    "            padding = child.padding\n",
    "            stride = child.stride\n",
    "            child_widened = convolution_block(in_h, in_w, in_channels*widening_factor, out_channels, kernel_size, padding, stride)\n",
    "            child_widened.conv_layer.weight.data[:, :in_channels, :, :] = child.conv_layer.weight.data\n",
    "#             child_widened.describe_adj_list([widened_parent_block if x == parent_block else x for x in child.in_adj], child.out_adj)\n",
    "#             self.int_to_node[child_no] = child_widened\n",
    "#             del self.node_to_int[child]\n",
    "#             self.node_to_int[child_widened] = child_no\n",
    "#             parent_out_adj.append(child_widened)\n",
    "#         widened_parent_block.describe_adj_list(parent_block.in_adj, parent_out_adj)\n",
    "    \n",
    "    def dfs(self, curr_node, visited, weight):\n",
    "        visited[curr_node] = weight\n",
    "        for child in self.adj_list[curr_node][1]:\n",
    "            if child not in visited:\n",
    "                kernel = 0\n",
    "                padding = 0\n",
    "                constant = 0\n",
    "                child_node = self.int_to_node[child]\n",
    "                ## make adjustments for concatenation\n",
    "                if isinstance(child_node, convolution_block) or isinstance(child_node, max_pool_node):\n",
    "                    kernel = child_node.kernel_size\n",
    "                    padding = child_node.padding\n",
    "                    constant = 1\n",
    "                self.dfs(child, visited, [weight[0]+kernel, weight[1]+padding, weight[2]+constant])\n",
    "        \n",
    "        \n",
    "    def get_descendant_vectors(self):\n",
    "        descs = {}\n",
    "        for curr_node in self.nodes:\n",
    "            visited = {}\n",
    "            self.dfs(curr_node, visited, [0, 0, 0])\n",
    "            del visited[curr_node] # remove root \n",
    "            descs[curr_node] = visited\n",
    "        return descs\n",
    "        \n",
    "    def skip_morph(self):\n",
    "        descs = self.get_descendant_vectors()\n",
    "        candidates = [(ans, des) for ans in descs for des in descs[ans] ]\n",
    "        no1, no2 = random.choice(candidates)\n",
    "        weight = descs[no1][no2]\n",
    "        #join outputs of node1 and node2 using a merge block\n",
    "        node_a = self.int_to_node[no1]\n",
    "        node_b = self.int_to_node[no2]\n",
    "        out_ch_1, out_h_1, out_w_1 = node_a.output_shape\n",
    "        out_ch_2, out_h_2, out_w_2 = node_b.output_shape\n",
    "#         print 'selected_nodes are ', no1, \"  \",no2\n",
    "#         print 'weight is   ', weight\n",
    "#         print(out_h_1, out_h_2, weight[0] - 2*weight[1] - weight[2])\n",
    "        assert(out_h_1 - out_h_2 == out_w_1 - out_w_2)\n",
    "        assert(out_h_1 - out_h_2 == weight[0] - 2*weight[1] - weight[2])\n",
    "\n",
    "        if weight[2] & 1 == 0:\n",
    "            weight[2] += 1\n",
    "            weight[0] += 1\n",
    "        weight[1] += (weight[2])/2\n",
    "        weight[2] -= 2*(weight[2]/2)\n",
    "        kernel_size = weight[0]\n",
    "        padding = weight[1]\n",
    "        stride = 1\n",
    "        new_conv = convolution_block(out_h_1, out_w_1, out_ch_1, out_ch_2, kernel_size, padding, stride)\n",
    "        new_add = add_node([new_conv, node_b], node_b.out_adj)\n",
    "        new_conv.describe_adj_list([node_a], [new_add])\n",
    "        new_conv.conv_layer.weight.data = torch.zeros(new_conv.conv_layer.weight.data.shape)\n",
    "        node_a.describe_adj_list(node_a.in_adj, node_a.out_adj+[new_conv])\n",
    "        for child_node in node_b.out_adj:\n",
    "            child_node.describe_adj_list([new_add if x==node_b else x for x in child_node.in_adj], child_node.out_adj)\n",
    "        node_b.describe_adj_list(node_b.in_adj, [new_add])\n",
    "        self.add_nodes_to_network([node_a, node_b, new_conv, new_add] + new_add.out_adj)\n",
    "        \n",
    "        \n",
    "        ###\n",
    "    \n",
    "    def visualize(self):\n",
    "        graph = Digraph('./images/arch', './images/arch.gv')\n",
    "        for no, curr_node in self.int_to_node.items():\n",
    "#             graph.node(str(no), str(type(curr_node)).split('__main__.')[1])\n",
    "            graph.node(str(no), str(self.node_to_int[curr_node]) + \" :: \" + repr(curr_node)[:200])\n",
    "        for no, li in self.adj_list.items():\n",
    "            for ch in li[1]:\n",
    "                graph.edge(str(no), str(ch))\n",
    "        graph.view()\n",
    "#         x = IFrame(\"./images/archgv.pdf\", width=600, height=300)\n",
    "#         print x\n",
    "    \n",
    "    def describe(self):\n",
    "        print 'Nodes: ', self.nodes\n",
    "        print 'Conv_blocks', self.conv_blocks\n",
    "        print 'Max_pool_blocks', self.max_pool_blocks\n",
    "        print 'Adj_list', self.adj_list\n",
    "        print 'Adj_mat', self.adj_mat\n",
    "        print 'int_to_node', self.int_to_node\n",
    "        print 'node_to_int', self.node_to_int\n",
    "        print 'Toposort', self.topsort\n",
    "        print 'node_dict', self.node_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():    \n",
    "    dummy = node((3, 32, 32), (3, 32, 32))\n",
    "    #     input_shape = (0, 0, 0), output_shape = (0, 0, 0)): # c, i1, i2\n",
    "    n1 = convolution_block(32, 32, 3, 4, 3)\n",
    "    #     in_h, in_w, in_channels, out_channels, kernel_size, padding = 0, stride = 1)\n",
    "    n2 = convolution_block(30, 30, 4, 3, 3)\n",
    "    dummy.describe_adj_list([], [n1])\n",
    "    n1.describe_adj_list([dummy], [n2])\n",
    "    n2.describe_adj_list([n1], [])\n",
    "    \n",
    "    \n",
    "    net = Network({0:[[], [1]], 1:[[0], [2]], 2: [[1], []]}, {0: dummy, 1:n1, 2:n2})\n",
    "    net.describe() \n",
    "    net.visualize()\n",
    "    for nan in net.nodes:\n",
    "        print net.int_to_node[nan].input_shape, ' -> ', nan, '->', net.int_to_node[nan].output_shape \n",
    "    for i in range(20):\n",
    "        print \"Iteration\", i\n",
    "        net.skip_morph()\n",
    "        net.deepen_morph()\n",
    "        net.widen_morph()\n",
    "        net.visualize()\n",
    "    net=addLinearLayers(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes:  [0, 1, 2]\n",
      "Conv_blocks [1, 2]\n",
      "Max_pool_blocks []\n",
      "Adj_list {0: [[], [1]], 1: [[0], [2]], 2: [[1], []]}\n",
      "Adj_mat {0: {0: 0, 1: 1, 2: 0}, 1: {0: 0, 1: 0, 2: 1}, 2: {0: 0, 1: 0, 2: 0}}\n",
      "int_to_node {0: 450 simple, 1: 451 conv, 2: 452 conv}\n",
      "node_to_int {450 simple: 0, 451 conv: 1, 452 conv: 2}\n",
      "Toposort [0, 1, 2]\n",
      "node_dict {'1': 451 conv, '2': 452 conv}\n",
      "(3, 32, 32)  ->  0 -> (3, 32, 32)\n",
      "(3, 32, 32)  ->  1 -> (4, 30, 30)\n",
      "(4, 30, 30)  ->  2 -> (3, 28, 28)\n",
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download=True, transform=transform)\n",
    "\n",
    "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#Training\n",
    "n_training_samples = 20000\n",
    "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "\n",
    "#Validation\n",
    "n_val_samples = 5000\n",
    "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 5000\n",
    "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
    "    return(train_loader)\n",
    "\n",
    "#Test and validation loaders have constant batch sizes, so we can define them directly\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def createLossAndOptimizer(net, learning_rate=0.001):\n",
    "    \n",
    "    #Loss function\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    #Optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    return(loss, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size=\", batch_size)\n",
    "    print(\"epochs=\", n_epochs)\n",
    "    print(\"learning_rate=\", learning_rate)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    #Get training data\n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Create our loss and optimizer functions\n",
    "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
    "    \n",
    "    #Time for printing\n",
    "    training_start_time = time.time()\n",
    "    \n",
    "    #Loop for n_epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "            \n",
    "            #Wrap them in a Variable object\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)\n",
    "            loss_size = loss(outputs, labels)\n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #for accuracy\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()  \n",
    "            \n",
    "            #Print statistics\n",
    "            running_loss += loss_size.data[0]\n",
    "            total_train_loss += loss_size.data[0]\n",
    "            \n",
    "            #Print every 10th batch of an epoch\n",
    "            if (i + 1) % (print_every + 1) == 0:\n",
    "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s Training Accuracy: {:.2f} \".format(\n",
    "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time, 100.*correct/total))\n",
    "                \n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #At the end of the epoch, do a pass on the validation set\n",
    "        total_val_loss = 0\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            #Wrap tensors in Variables\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data[0]\n",
    "            \n",
    "            #for val acc\n",
    "            _, predicted = val_outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()  \n",
    "            \n",
    "        print \"Validation Acc\", 100*val_correct/val_total      \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "        \n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_train():    \n",
    "    dummy = node((3, 32, 32), (3, 32, 32))\n",
    "#     input_shape = (0, 0, 0), output_shape = (0, 0, 0)): # c, i1, i2\n",
    "    n1 = convolution_block(32, 32, 3, 18, 3)\n",
    "#     in_h, in_w, in_channels, out_channels, kernel_size, padding = 0, stride = 1)\n",
    "    p = max_pool_node(30, 30, 18, 2)\n",
    "    n2 = convolution_block(29, 29, 18, 3, 3)\n",
    "    dummy.describe_adj_list([], [n1])\n",
    "    n1.describe_adj_list([dummy], [p])\n",
    "    p.describe_adj_list([n1], [n2])\n",
    "    n2.describe_adj_list([p], [])\n",
    "    net = Network({0:[[], [1]], 1:[[0], [2]], 2: [[1], [3]], 3:[[2], []]}, {0: dummy, 1:n1, 2: p, 3:n2})\n",
    "    net.createModel()\n",
    "#     net.describe()                    \n",
    "#     net.visualize()\n",
    "#     print (net.int_to_node[net.topsort[-1]].out_shape())\n",
    "    net=addLinearLayers(net)\n",
    "#     print list(n1.parameters())\n",
    "#     for nan in net.nodes:\n",
    "#         print net.int_to_node[nan].input_shape, ' -> ', nan, '->', net.int_to_node[nan].output_shape \n",
    "#     for i in range(20):\n",
    "#         net.skip_morph()\n",
    "#         net.deepen_morph()\n",
    "#         net.widen_morph()\n",
    "#     net.visualize()\n",
    "#     print list(net.parameters())\n",
    "#     print issubclass(type(net), nn.Module)\n",
    "    trainNet(net, batch_size=32, n_epochs=30, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== HYPERPARAMETERS =====\n",
      "('batch_size=', 32)\n",
      "('epochs=', 30)\n",
      "('learning_rate=', 0.0001)\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:54: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:55: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 10% \t train_loss: 2.32 took: 3.11s Training Accuracy: 13.84 \n",
      "Epoch 1, 20% \t train_loss: 2.27 took: 2.78s Training Accuracy: 15.70 \n",
      "Epoch 1, 30% \t train_loss: 2.22 took: 2.82s Training Accuracy: 17.58 \n",
      "Epoch 1, 40% \t train_loss: 2.18 took: 2.78s Training Accuracy: 19.32 \n",
      "Epoch 1, 50% \t train_loss: 2.14 took: 2.88s Training Accuracy: 20.96 \n",
      "Epoch 1, 60% \t train_loss: 2.10 took: 2.78s Training Accuracy: 22.03 \n",
      "Epoch 1, 70% \t train_loss: 2.03 took: 2.78s Training Accuracy: 23.37 \n",
      "Epoch 1, 80% \t train_loss: 2.01 took: 2.79s Training Accuracy: 24.12 \n",
      "Epoch 1, 90% \t train_loss: 1.95 took: 2.84s Training Accuracy: 25.24 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:78: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Acc 36\n",
      "Validation loss = 1.85\n",
      "Epoch 2, 10% \t train_loss: 1.86 took: 2.84s Training Accuracy: 36.90 \n",
      "Epoch 2, 20% \t train_loss: 1.84 took: 2.83s Training Accuracy: 36.88 \n",
      "Epoch 2, 30% \t train_loss: 1.80 took: 2.90s Training Accuracy: 37.67 \n",
      "Epoch 2, 40% \t train_loss: 1.76 took: 2.81s Training Accuracy: 38.13 \n",
      "Epoch 2, 50% \t train_loss: 1.72 took: 2.85s Training Accuracy: 38.64 \n",
      "Epoch 2, 60% \t train_loss: 1.74 took: 2.83s Training Accuracy: 38.84 \n",
      "Epoch 2, 70% \t train_loss: 1.71 took: 2.81s Training Accuracy: 39.22 \n",
      "Epoch 2, 80% \t train_loss: 1.70 took: 2.79s Training Accuracy: 39.30 \n",
      "Epoch 2, 90% \t train_loss: 1.69 took: 2.82s Training Accuracy: 39.50 \n",
      "Validation Acc 42\n",
      "Validation loss = 1.60\n",
      "Epoch 3, 10% \t train_loss: 1.65 took: 2.91s Training Accuracy: 42.66 \n",
      "Epoch 3, 20% \t train_loss: 1.63 took: 2.81s Training Accuracy: 43.03 \n",
      "Epoch 3, 30% \t train_loss: 1.65 took: 2.80s Training Accuracy: 42.66 \n",
      "Epoch 3, 40% \t train_loss: 1.63 took: 2.81s Training Accuracy: 43.04 \n",
      "Epoch 3, 50% \t train_loss: 1.61 took: 2.82s Training Accuracy: 43.08 \n",
      "Epoch 3, 60% \t train_loss: 1.62 took: 2.81s Training Accuracy: 43.04 \n",
      "Epoch 3, 70% \t train_loss: 1.63 took: 2.83s Training Accuracy: 43.10 \n",
      "Epoch 3, 80% \t train_loss: 1.63 took: 2.83s Training Accuracy: 43.02 \n",
      "Epoch 3, 90% \t train_loss: 1.59 took: 2.84s Training Accuracy: 43.04 \n",
      "Validation Acc 44\n",
      "Validation loss = 1.53\n",
      "Epoch 4, 10% \t train_loss: 1.60 took: 2.91s Training Accuracy: 43.65 \n",
      "Epoch 4, 20% \t train_loss: 1.60 took: 2.83s Training Accuracy: 44.54 \n",
      "Epoch 4, 30% \t train_loss: 1.59 took: 3.26s Training Accuracy: 44.33 \n",
      "Epoch 4, 40% \t train_loss: 1.58 took: 2.83s Training Accuracy: 44.13 \n",
      "Epoch 4, 50% \t train_loss: 1.54 took: 2.81s Training Accuracy: 44.62 \n",
      "Epoch 4, 60% \t train_loss: 1.58 took: 2.83s Training Accuracy: 44.57 \n",
      "Epoch 4, 70% \t train_loss: 1.52 took: 2.80s Training Accuracy: 44.85 \n",
      "Epoch 4, 80% \t train_loss: 1.53 took: 2.80s Training Accuracy: 45.03 \n",
      "Epoch 4, 90% \t train_loss: 1.56 took: 2.82s Training Accuracy: 45.07 \n",
      "Validation Acc 46\n",
      "Validation loss = 1.51\n",
      "Epoch 5, 10% \t train_loss: 1.53 took: 2.85s Training Accuracy: 46.43 \n",
      "Epoch 5, 20% \t train_loss: 1.54 took: 3.36s Training Accuracy: 46.03 \n",
      "Epoch 5, 30% \t train_loss: 1.53 took: 2.92s Training Accuracy: 46.28 \n",
      "Epoch 5, 40% \t train_loss: 1.56 took: 2.84s Training Accuracy: 45.86 \n",
      "Epoch 5, 50% \t train_loss: 1.56 took: 2.80s Training Accuracy: 45.72 \n",
      "Epoch 5, 60% \t train_loss: 1.52 took: 2.85s Training Accuracy: 45.68 \n",
      "Epoch 5, 70% \t train_loss: 1.51 took: 2.77s Training Accuracy: 45.95 \n",
      "Epoch 5, 80% \t train_loss: 1.52 took: 2.79s Training Accuracy: 45.93 \n",
      "Epoch 5, 90% \t train_loss: 1.59 took: 2.78s Training Accuracy: 45.84 \n",
      "Validation Acc 47\n",
      "Validation loss = 1.48\n",
      "Epoch 6, 10% \t train_loss: 1.52 took: 3.25s Training Accuracy: 46.53 \n",
      "Epoch 6, 20% \t train_loss: 1.50 took: 3.21s Training Accuracy: 46.92 \n",
      "Epoch 6, 30% \t train_loss: 1.49 took: 3.15s Training Accuracy: 47.09 \n",
      "Epoch 6, 40% \t train_loss: 1.51 took: 2.91s Training Accuracy: 47.20 \n",
      "Epoch 6, 50% \t train_loss: 1.49 took: 2.81s Training Accuracy: 47.12 \n",
      "Epoch 6, 60% \t train_loss: 1.52 took: 2.79s Training Accuracy: 47.07 \n",
      "Epoch 6, 70% \t train_loss: 1.51 took: 2.77s Training Accuracy: 47.02 \n",
      "Epoch 6, 80% \t train_loss: 1.54 took: 2.83s Training Accuracy: 46.89 \n",
      "Epoch 6, 90% \t train_loss: 1.53 took: 2.78s Training Accuracy: 46.79 \n",
      "Validation Acc 47\n",
      "Validation loss = 1.46\n",
      "Epoch 7, 10% \t train_loss: 1.47 took: 3.35s Training Accuracy: 48.91 \n",
      "Epoch 7, 20% \t train_loss: 1.48 took: 3.13s Training Accuracy: 48.41 \n",
      "Epoch 7, 30% \t train_loss: 1.47 took: 3.17s Training Accuracy: 48.36 \n",
      "Epoch 7, 40% \t train_loss: 1.52 took: 3.38s Training Accuracy: 47.78 \n",
      "Epoch 7, 50% \t train_loss: 1.49 took: 3.47s Training Accuracy: 47.82 \n",
      "Epoch 7, 60% \t train_loss: 1.50 took: 3.13s Training Accuracy: 47.92 \n",
      "Epoch 7, 70% \t train_loss: 1.49 took: 2.78s Training Accuracy: 47.82 \n",
      "Epoch 7, 80% \t train_loss: 1.48 took: 2.76s Training Accuracy: 48.08 \n",
      "Epoch 7, 90% \t train_loss: 1.51 took: 2.97s Training Accuracy: 47.96 \n",
      "Validation Acc 47\n",
      "Validation loss = 1.46\n",
      "Epoch 8, 10% \t train_loss: 1.50 took: 2.96s Training Accuracy: 47.62 \n",
      "Epoch 8, 20% \t train_loss: 1.46 took: 2.79s Training Accuracy: 48.49 \n",
      "Epoch 8, 30% \t train_loss: 1.48 took: 3.02s Training Accuracy: 48.36 \n",
      "Epoch 8, 40% \t train_loss: 1.48 took: 3.37s Training Accuracy: 48.34 \n",
      "Epoch 8, 50% \t train_loss: 1.48 took: 3.77s Training Accuracy: 48.31 \n",
      "Epoch 8, 60% \t train_loss: 1.43 took: 5.76s Training Accuracy: 48.25 \n",
      "Epoch 8, 70% \t train_loss: 1.49 took: 4.50s Training Accuracy: 48.19 \n",
      "Epoch 8, 80% \t train_loss: 1.47 took: 4.98s Training Accuracy: 48.13 \n",
      "Epoch 8, 90% \t train_loss: 1.47 took: 4.59s Training Accuracy: 48.15 \n",
      "Validation Acc 48\n",
      "Validation loss = 1.44\n",
      "Epoch 9, 10% \t train_loss: 1.46 took: 3.78s Training Accuracy: 49.60 \n",
      "Epoch 9, 20% \t train_loss: 1.46 took: 3.08s Training Accuracy: 48.93 \n",
      "Epoch 9, 30% \t train_loss: 1.46 took: 3.30s Training Accuracy: 49.37 \n",
      "Epoch 9, 40% \t train_loss: 1.44 took: 3.00s Training Accuracy: 49.17 \n",
      "Epoch 9, 50% \t train_loss: 1.44 took: 3.65s Training Accuracy: 49.04 \n",
      "Epoch 9, 60% \t train_loss: 1.50 took: 3.12s Training Accuracy: 48.74 \n",
      "Epoch 9, 70% \t train_loss: 1.46 took: 3.60s Training Accuracy: 48.84 \n",
      "Epoch 9, 80% \t train_loss: 1.50 took: 3.15s Training Accuracy: 48.62 \n",
      "Epoch 9, 90% \t train_loss: 1.46 took: 2.92s Training Accuracy: 48.64 \n",
      "Validation Acc 48\n",
      "Validation loss = 1.45\n",
      "Epoch 10, 10% \t train_loss: 1.48 took: 2.91s Training Accuracy: 48.21 \n",
      "Epoch 10, 20% \t train_loss: 1.43 took: 2.79s Training Accuracy: 48.66 \n",
      "Epoch 10, 30% \t train_loss: 1.46 took: 2.82s Training Accuracy: 49.11 \n",
      "Epoch 10, 40% \t train_loss: 1.40 took: 2.83s Training Accuracy: 49.83 \n",
      "Epoch 10, 50% \t train_loss: 1.46 took: 2.81s Training Accuracy: 49.55 \n",
      "Epoch 10, 60% \t train_loss: 1.44 took: 2.80s Training Accuracy: 49.34 \n",
      "Epoch 10, 70% \t train_loss: 1.44 took: 2.78s Training Accuracy: 49.56 \n",
      "Epoch 10, 80% \t train_loss: 1.44 took: 2.80s Training Accuracy: 49.63 \n",
      "Epoch 10, 90% \t train_loss: 1.44 took: 2.78s Training Accuracy: 49.51 \n",
      "Validation Acc 48\n",
      "Validation loss = 1.42\n",
      "Epoch 11, 10% \t train_loss: 1.45 took: 2.88s Training Accuracy: 47.97 \n",
      "Epoch 11, 20% \t train_loss: 1.44 took: 2.83s Training Accuracy: 49.55 \n",
      "Epoch 11, 30% \t train_loss: 1.45 took: 2.78s Training Accuracy: 49.29 \n",
      "Epoch 11, 40% \t train_loss: 1.43 took: 2.74s Training Accuracy: 49.90 \n",
      "Epoch 11, 50% \t train_loss: 1.43 took: 2.78s Training Accuracy: 50.00 \n",
      "Epoch 11, 60% \t train_loss: 1.46 took: 2.81s Training Accuracy: 49.97 \n",
      "Epoch 11, 70% \t train_loss: 1.40 took: 2.79s Training Accuracy: 50.03 \n",
      "Epoch 11, 80% \t train_loss: 1.42 took: 2.79s Training Accuracy: 50.03 \n",
      "Epoch 11, 90% \t train_loss: 1.42 took: 3.26s Training Accuracy: 50.04 \n",
      "Validation Acc 48\n",
      "Validation loss = 1.44\n",
      "Epoch 12, 10% \t train_loss: 1.41 took: 3.01s Training Accuracy: 51.79 \n",
      "Epoch 12, 20% \t train_loss: 1.40 took: 2.80s Training Accuracy: 50.77 \n",
      "Epoch 12, 30% \t train_loss: 1.44 took: 2.73s Training Accuracy: 50.58 \n",
      "Epoch 12, 40% \t train_loss: 1.43 took: 2.81s Training Accuracy: 50.67 \n",
      "Epoch 12, 50% \t train_loss: 1.40 took: 2.82s Training Accuracy: 50.74 \n",
      "Epoch 12, 60% \t train_loss: 1.42 took: 3.36s Training Accuracy: 50.46 \n",
      "Epoch 12, 70% \t train_loss: 1.42 took: 2.79s Training Accuracy: 50.24 \n",
      "Epoch 12, 80% \t train_loss: 1.46 took: 2.76s Training Accuracy: 50.17 \n",
      "Epoch 12, 90% \t train_loss: 1.44 took: 2.75s Training Accuracy: 49.99 \n",
      "Validation Acc 49\n",
      "Validation loss = 1.42\n",
      "Epoch 13, 10% \t train_loss: 1.37 took: 2.87s Training Accuracy: 52.33 \n",
      "Epoch 13, 20% \t train_loss: 1.40 took: 2.82s Training Accuracy: 51.54 \n",
      "Epoch 13, 30% \t train_loss: 1.43 took: 2.81s Training Accuracy: 50.99 \n",
      "Epoch 13, 40% \t train_loss: 1.42 took: 2.75s Training Accuracy: 51.09 \n",
      "Epoch 13, 50% \t train_loss: 1.43 took: 2.80s Training Accuracy: 50.76 \n",
      "Epoch 13, 60% \t train_loss: 1.41 took: 2.81s Training Accuracy: 50.60 \n",
      "Epoch 13, 70% \t train_loss: 1.43 took: 2.80s Training Accuracy: 50.46 \n",
      "Epoch 13, 80% \t train_loss: 1.33 took: 2.79s Training Accuracy: 51.03 \n",
      "Epoch 13, 90% \t train_loss: 1.42 took: 2.83s Training Accuracy: 50.75 \n",
      "Validation Acc 49\n",
      "Validation loss = 1.40\n",
      "Epoch 14, 10% \t train_loss: 1.41 took: 2.90s Training Accuracy: 51.24 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, 20% \t train_loss: 1.38 took: 2.78s Training Accuracy: 52.08 \n",
      "Epoch 14, 30% \t train_loss: 1.42 took: 2.79s Training Accuracy: 51.41 \n",
      "Epoch 14, 40% \t train_loss: 1.40 took: 2.77s Training Accuracy: 51.26 \n",
      "Epoch 14, 50% \t train_loss: 1.39 took: 3.59s Training Accuracy: 51.21 \n",
      "Epoch 14, 60% \t train_loss: 1.40 took: 2.95s Training Accuracy: 51.29 \n",
      "Epoch 14, 70% \t train_loss: 1.43 took: 3.17s Training Accuracy: 51.06 \n",
      "Epoch 14, 80% \t train_loss: 1.37 took: 3.31s Training Accuracy: 51.26 \n",
      "Epoch 14, 90% \t train_loss: 1.38 took: 3.50s Training Accuracy: 51.23 \n",
      "Validation Acc 49\n",
      "Validation loss = 1.42\n",
      "Epoch 15, 10% \t train_loss: 1.40 took: 2.84s Training Accuracy: 51.34 \n",
      "Epoch 15, 20% \t train_loss: 1.38 took: 2.82s Training Accuracy: 52.01 \n",
      "Epoch 15, 30% \t train_loss: 1.35 took: 2.77s Training Accuracy: 51.98 \n",
      "Epoch 15, 40% \t train_loss: 1.36 took: 2.83s Training Accuracy: 52.24 \n",
      "Epoch 15, 50% \t train_loss: 1.36 took: 2.97s Training Accuracy: 52.24 \n",
      "Epoch 15, 60% \t train_loss: 1.41 took: 2.89s Training Accuracy: 51.74 \n",
      "Epoch 15, 70% \t train_loss: 1.39 took: 2.88s Training Accuracy: 51.76 \n",
      "Epoch 15, 80% \t train_loss: 1.37 took: 2.93s Training Accuracy: 51.79 \n",
      "Epoch 15, 90% \t train_loss: 1.36 took: 3.20s Training Accuracy: 51.91 \n",
      "Validation Acc 49\n",
      "Validation loss = 1.41\n",
      "Epoch 16, 10% \t train_loss: 1.37 took: 2.83s Training Accuracy: 52.13 \n",
      "Epoch 16, 20% \t train_loss: 1.37 took: 2.98s Training Accuracy: 52.58 \n",
      "Epoch 16, 30% \t train_loss: 1.39 took: 2.75s Training Accuracy: 52.33 \n",
      "Epoch 16, 40% \t train_loss: 1.39 took: 2.77s Training Accuracy: 51.75 \n",
      "Epoch 16, 50% \t train_loss: 1.35 took: 2.80s Training Accuracy: 52.24 \n",
      "Epoch 16, 60% \t train_loss: 1.36 took: 2.77s Training Accuracy: 52.17 \n",
      "Epoch 16, 70% \t train_loss: 1.34 took: 2.80s Training Accuracy: 52.18 \n",
      "Epoch 16, 80% \t train_loss: 1.37 took: 2.84s Training Accuracy: 52.42 \n",
      "Epoch 16, 90% \t train_loss: 1.37 took: 2.77s Training Accuracy: 52.50 \n",
      "Validation Acc 49\n",
      "Validation loss = 1.41\n",
      "Epoch 17, 10% \t train_loss: 1.37 took: 2.86s Training Accuracy: 51.24 \n",
      "Epoch 17, 20% \t train_loss: 1.33 took: 2.77s Training Accuracy: 52.18 \n",
      "Epoch 17, 30% \t train_loss: 1.36 took: 2.80s Training Accuracy: 52.40 \n",
      "Epoch 17, 40% \t train_loss: 1.36 took: 3.18s Training Accuracy: 52.50 \n",
      "Epoch 17, 50% \t train_loss: 1.34 took: 3.39s Training Accuracy: 52.73 \n",
      "Epoch 17, 60% \t train_loss: 1.38 took: 3.18s Training Accuracy: 52.64 \n",
      "Epoch 17, 70% \t train_loss: 1.36 took: 2.78s Training Accuracy: 52.54 \n",
      "Epoch 17, 80% \t train_loss: 1.39 took: 3.15s Training Accuracy: 52.34 \n",
      "Epoch 17, 90% \t train_loss: 1.37 took: 3.00s Training Accuracy: 52.27 \n",
      "Validation Acc 49\n",
      "Validation loss = 1.40\n",
      "Epoch 18, 10% \t train_loss: 1.34 took: 2.90s Training Accuracy: 51.88 \n",
      "Epoch 18, 20% \t train_loss: 1.36 took: 2.80s Training Accuracy: 52.68 \n",
      "Epoch 18, 30% \t train_loss: 1.32 took: 2.81s Training Accuracy: 53.04 \n",
      "Epoch 18, 40% \t train_loss: 1.37 took: 2.75s Training Accuracy: 53.06 \n",
      "Epoch 18, 50% \t train_loss: 1.36 took: 2.77s Training Accuracy: 52.97 \n",
      "Epoch 18, 60% \t train_loss: 1.34 took: 2.80s Training Accuracy: 53.12 \n",
      "Epoch 18, 70% \t train_loss: 1.37 took: 2.77s Training Accuracy: 52.88 \n",
      "Epoch 18, 80% \t train_loss: 1.37 took: 2.73s Training Accuracy: 52.74 \n",
      "Epoch 18, 90% \t train_loss: 1.37 took: 2.74s Training Accuracy: 52.75 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.41\n",
      "Epoch 19, 10% \t train_loss: 1.33 took: 2.83s Training Accuracy: 55.31 \n",
      "Epoch 19, 20% \t train_loss: 1.36 took: 2.79s Training Accuracy: 53.97 \n",
      "Epoch 19, 30% \t train_loss: 1.33 took: 2.75s Training Accuracy: 53.77 \n",
      "Epoch 19, 40% \t train_loss: 1.32 took: 2.77s Training Accuracy: 53.92 \n",
      "Epoch 19, 50% \t train_loss: 1.36 took: 2.87s Training Accuracy: 53.68 \n",
      "Epoch 19, 60% \t train_loss: 1.37 took: 2.78s Training Accuracy: 53.58 \n",
      "Epoch 19, 70% \t train_loss: 1.36 took: 2.82s Training Accuracy: 53.25 \n",
      "Epoch 19, 80% \t train_loss: 1.35 took: 2.78s Training Accuracy: 52.97 \n",
      "Epoch 19, 90% \t train_loss: 1.31 took: 2.79s Training Accuracy: 53.08 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.39\n",
      "Epoch 20, 10% \t train_loss: 1.39 took: 2.89s Training Accuracy: 52.03 \n",
      "Epoch 20, 20% \t train_loss: 1.37 took: 2.98s Training Accuracy: 51.74 \n",
      "Epoch 20, 30% \t train_loss: 1.30 took: 2.81s Training Accuracy: 52.78 \n",
      "Epoch 20, 40% \t train_loss: 1.36 took: 2.83s Training Accuracy: 52.94 \n",
      "Epoch 20, 50% \t train_loss: 1.30 took: 2.76s Training Accuracy: 53.50 \n",
      "Epoch 20, 60% \t train_loss: 1.30 took: 2.82s Training Accuracy: 53.65 \n",
      "Epoch 20, 70% \t train_loss: 1.35 took: 2.80s Training Accuracy: 53.50 \n",
      "Epoch 20, 80% \t train_loss: 1.33 took: 2.79s Training Accuracy: 53.62 \n",
      "Epoch 20, 90% \t train_loss: 1.35 took: 2.75s Training Accuracy: 53.60 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.37\n",
      "Epoch 21, 10% \t train_loss: 1.29 took: 2.94s Training Accuracy: 53.92 \n",
      "Epoch 21, 20% \t train_loss: 1.33 took: 2.79s Training Accuracy: 53.82 \n",
      "Epoch 21, 30% \t train_loss: 1.32 took: 2.77s Training Accuracy: 54.05 \n",
      "Epoch 21, 40% \t train_loss: 1.33 took: 2.78s Training Accuracy: 54.02 \n",
      "Epoch 21, 50% \t train_loss: 1.32 took: 2.81s Training Accuracy: 53.93 \n",
      "Epoch 21, 60% \t train_loss: 1.30 took: 2.76s Training Accuracy: 54.16 \n",
      "Epoch 21, 70% \t train_loss: 1.30 took: 2.80s Training Accuracy: 54.40 \n",
      "Epoch 21, 80% \t train_loss: 1.36 took: 2.80s Training Accuracy: 54.13 \n",
      "Epoch 21, 90% \t train_loss: 1.32 took: 2.83s Training Accuracy: 54.13 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.38\n",
      "Epoch 22, 10% \t train_loss: 1.29 took: 2.90s Training Accuracy: 55.01 \n",
      "Epoch 22, 20% \t train_loss: 1.31 took: 2.78s Training Accuracy: 55.16 \n",
      "Epoch 22, 30% \t train_loss: 1.32 took: 3.29s Training Accuracy: 54.73 \n",
      "Epoch 22, 40% \t train_loss: 1.29 took: 2.80s Training Accuracy: 54.51 \n",
      "Epoch 22, 50% \t train_loss: 1.30 took: 3.08s Training Accuracy: 54.46 \n",
      "Epoch 22, 60% \t train_loss: 1.30 took: 2.75s Training Accuracy: 54.46 \n",
      "Epoch 22, 70% \t train_loss: 1.32 took: 2.75s Training Accuracy: 54.41 \n",
      "Epoch 22, 80% \t train_loss: 1.32 took: 2.79s Training Accuracy: 54.40 \n",
      "Epoch 22, 90% \t train_loss: 1.35 took: 2.77s Training Accuracy: 54.26 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.39\n",
      "Epoch 23, 10% \t train_loss: 1.27 took: 2.84s Training Accuracy: 56.65 \n",
      "Epoch 23, 20% \t train_loss: 1.31 took: 2.86s Training Accuracy: 55.51 \n",
      "Epoch 23, 30% \t train_loss: 1.28 took: 2.89s Training Accuracy: 55.14 \n",
      "Epoch 23, 40% \t train_loss: 1.31 took: 2.95s Training Accuracy: 54.90 \n",
      "Epoch 23, 50% \t train_loss: 1.30 took: 3.16s Training Accuracy: 54.98 \n",
      "Epoch 23, 60% \t train_loss: 1.31 took: 2.81s Training Accuracy: 54.79 \n",
      "Epoch 23, 70% \t train_loss: 1.33 took: 2.80s Training Accuracy: 54.61 \n",
      "Epoch 23, 80% \t train_loss: 1.31 took: 2.78s Training Accuracy: 54.62 \n",
      "Epoch 23, 90% \t train_loss: 1.32 took: 2.75s Training Accuracy: 54.62 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.38\n",
      "Epoch 24, 10% \t train_loss: 1.29 took: 2.90s Training Accuracy: 56.00 \n",
      "Epoch 24, 20% \t train_loss: 1.30 took: 2.79s Training Accuracy: 56.08 \n",
      "Epoch 24, 30% \t train_loss: 1.28 took: 3.02s Training Accuracy: 55.57 \n",
      "Epoch 24, 40% \t train_loss: 1.27 took: 3.36s Training Accuracy: 55.64 \n",
      "Epoch 24, 50% \t train_loss: 1.31 took: 3.78s Training Accuracy: 55.69 \n",
      "Epoch 24, 60% \t train_loss: 1.31 took: 3.88s Training Accuracy: 55.32 \n",
      "Epoch 24, 70% \t train_loss: 1.31 took: 2.80s Training Accuracy: 55.14 \n",
      "Epoch 24, 80% \t train_loss: 1.28 took: 2.77s Training Accuracy: 55.24 \n",
      "Epoch 24, 90% \t train_loss: 1.31 took: 2.77s Training Accuracy: 55.12 \n",
      "Validation Acc 51\n",
      "Validation loss = 1.37\n",
      "Epoch 25, 10% \t train_loss: 1.28 took: 2.90s Training Accuracy: 56.45 \n",
      "Epoch 25, 20% \t train_loss: 1.29 took: 2.80s Training Accuracy: 55.53 \n",
      "Epoch 25, 30% \t train_loss: 1.24 took: 2.79s Training Accuracy: 55.85 \n",
      "Epoch 25, 40% \t train_loss: 1.30 took: 2.84s Training Accuracy: 55.08 \n",
      "Epoch 25, 50% \t train_loss: 1.32 took: 2.79s Training Accuracy: 55.03 \n",
      "Epoch 25, 60% \t train_loss: 1.29 took: 2.77s Training Accuracy: 54.89 \n",
      "Epoch 25, 70% \t train_loss: 1.27 took: 2.81s Training Accuracy: 55.05 \n",
      "Epoch 25, 80% \t train_loss: 1.28 took: 2.78s Training Accuracy: 55.10 \n",
      "Epoch 25, 90% \t train_loss: 1.27 took: 2.91s Training Accuracy: 55.28 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.38\n",
      "Epoch 26, 10% \t train_loss: 1.27 took: 2.90s Training Accuracy: 56.99 \n",
      "Epoch 26, 20% \t train_loss: 1.31 took: 2.79s Training Accuracy: 55.68 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, 30% \t train_loss: 1.28 took: 2.76s Training Accuracy: 55.85 \n",
      "Epoch 26, 40% \t train_loss: 1.29 took: 2.76s Training Accuracy: 55.84 \n",
      "Epoch 26, 50% \t train_loss: 1.27 took: 2.80s Training Accuracy: 55.73 \n",
      "Epoch 26, 60% \t train_loss: 1.28 took: 2.80s Training Accuracy: 55.78 \n",
      "Epoch 26, 70% \t train_loss: 1.22 took: 2.80s Training Accuracy: 55.91 \n",
      "Epoch 26, 80% \t train_loss: 1.29 took: 2.79s Training Accuracy: 55.75 \n",
      "Epoch 26, 90% \t train_loss: 1.26 took: 2.80s Training Accuracy: 55.74 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.39\n",
      "Epoch 27, 10% \t train_loss: 1.24 took: 2.86s Training Accuracy: 56.40 \n",
      "Epoch 27, 20% \t train_loss: 1.27 took: 2.79s Training Accuracy: 56.30 \n",
      "Epoch 27, 30% \t train_loss: 1.26 took: 2.79s Training Accuracy: 56.42 \n",
      "Epoch 27, 40% \t train_loss: 1.27 took: 2.77s Training Accuracy: 56.41 \n",
      "Epoch 27, 50% \t train_loss: 1.28 took: 2.82s Training Accuracy: 56.37 \n",
      "Epoch 27, 60% \t train_loss: 1.27 took: 2.81s Training Accuracy: 56.10 \n",
      "Epoch 27, 70% \t train_loss: 1.25 took: 2.74s Training Accuracy: 56.09 \n",
      "Epoch 27, 80% \t train_loss: 1.28 took: 2.78s Training Accuracy: 56.08 \n",
      "Epoch 27, 90% \t train_loss: 1.28 took: 2.79s Training Accuracy: 56.08 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.38\n",
      "Epoch 28, 10% \t train_loss: 1.24 took: 2.88s Training Accuracy: 56.20 \n",
      "Epoch 28, 20% \t train_loss: 1.26 took: 2.81s Training Accuracy: 56.03 \n",
      "Epoch 28, 30% \t train_loss: 1.24 took: 2.79s Training Accuracy: 56.37 \n",
      "Epoch 28, 40% \t train_loss: 1.25 took: 2.80s Training Accuracy: 56.65 \n",
      "Epoch 28, 50% \t train_loss: 1.25 took: 2.81s Training Accuracy: 56.52 \n",
      "Epoch 28, 60% \t train_loss: 1.24 took: 3.25s Training Accuracy: 56.52 \n",
      "Epoch 28, 70% \t train_loss: 1.33 took: 4.14s Training Accuracy: 56.01 \n",
      "Epoch 28, 80% \t train_loss: 1.27 took: 2.80s Training Accuracy: 55.87 \n",
      "Epoch 28, 90% \t train_loss: 1.24 took: 2.82s Training Accuracy: 55.95 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.37\n",
      "Epoch 29, 10% \t train_loss: 1.24 took: 2.92s Training Accuracy: 56.85 \n",
      "Epoch 29, 20% \t train_loss: 1.27 took: 2.86s Training Accuracy: 56.20 \n",
      "Epoch 29, 30% \t train_loss: 1.26 took: 2.78s Training Accuracy: 56.17 \n",
      "Epoch 29, 40% \t train_loss: 1.24 took: 2.83s Training Accuracy: 56.44 \n",
      "Epoch 29, 50% \t train_loss: 1.24 took: 2.80s Training Accuracy: 56.37 \n",
      "Epoch 29, 60% \t train_loss: 1.24 took: 2.95s Training Accuracy: 56.34 \n",
      "Epoch 29, 70% \t train_loss: 1.29 took: 2.84s Training Accuracy: 56.12 \n",
      "Epoch 29, 80% \t train_loss: 1.28 took: 2.88s Training Accuracy: 55.91 \n",
      "Epoch 29, 90% \t train_loss: 1.22 took: 2.85s Training Accuracy: 56.08 \n",
      "Validation Acc 50\n",
      "Validation loss = 1.38\n",
      "Epoch 30, 10% \t train_loss: 1.28 took: 2.92s Training Accuracy: 54.91 \n",
      "Epoch 30, 20% \t train_loss: 1.20 took: 2.82s Training Accuracy: 56.32 \n",
      "Epoch 30, 30% \t train_loss: 1.22 took: 2.86s Training Accuracy: 57.04 \n",
      "Epoch 30, 40% \t train_loss: 1.24 took: 2.85s Training Accuracy: 57.27 \n",
      "Epoch 30, 50% \t train_loss: 1.24 took: 2.83s Training Accuracy: 57.09 \n",
      "Epoch 30, 60% \t train_loss: 1.26 took: 2.82s Training Accuracy: 56.95 \n",
      "Epoch 30, 70% \t train_loss: 1.25 took: 2.88s Training Accuracy: 57.01 \n",
      "Epoch 30, 80% \t train_loss: 1.22 took: 2.83s Training Accuracy: 57.01 \n",
      "Epoch 30, 90% \t train_loss: 1.25 took: 2.80s Training Accuracy: 56.87 \n",
      "Validation Acc 51\n",
      "Validation loss = 1.37\n",
      "Training finished, took 985.54s\n"
     ]
    }
   ],
   "source": [
    "dummy_train()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class hill_climbing:\n",
    "    def __init__(X, Y, iterations = 100, init_network = None):\n",
    "        ## If init network is not provided, provide facility to begin with random network\n",
    "        # X and Y are data and labels\n",
    "        self.iterations = iterations \n",
    "        if init_network:\n",
    "            self.net = init_network\n",
    "        else:\n",
    "            self.net = random_network()\n",
    "    \n",
    "    def plot_loss_graph(self):\n",
    "        pass\n",
    "    \n",
    "    def visualize_tree_of_networks(self):\n",
    "        pass\n",
    "    \n",
    "    def morphism(self):\n",
    "        import random\n",
    "        actions = {'deepen': self.net.deepen_morph, \n",
    "                   'widen': self.net.widen_morph, \n",
    "                   'skip-connection': self.net.skip_morph }\n",
    "        choice = random.choice(actions)\n",
    "        actions[choice]()\n",
    "        \n",
    "    def start(self):\n",
    "        for iteration in range(iterations):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_climbing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
