{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from graphviz import Digraph\n",
    "import random\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cb0feb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 53\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node(object):\n",
    "    no =0\n",
    "#     node_type = 'simple'\n",
    "    def __init__(self, input_shape = (0, 0, 0), output_shape = (0, 0, 0)): # c, i1, i2\n",
    "        node.no += 1\n",
    "        self.no = node.no\n",
    "        self.in_adj = []\n",
    "        self.out_adj = []\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "    \n",
    "#     def set_shape(self, input_shape, output_shape):\n",
    "#         self.input_shape = input_shape\n",
    "#         self.output_shape = output_shape\n",
    "        \n",
    "\n",
    "    def describe_adj_list(self, in_adj, out_adj):\n",
    "        self.in_adj = in_adj\n",
    "        self.out_adj = out_adj\n",
    "\n",
    "    def out_shape(self):\n",
    "        pass\n",
    "    \n",
    "    def remove(self):\n",
    "        ## needed in graph class\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.no) + \" \" + str(type(self)) \n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convolution_block(nn.Module, node):\n",
    "    def __init__(self, in_channels, in_h, in_w, out_channels, kernel_size, padding = 0, stride = 1):\n",
    "        super(convolution_block, self).__init__()\n",
    "        node.__init__(self, (in_channels, in_h, in_w))\n",
    "        self.in_channels  = in_channels\n",
    "        self.out_channels = out_channels \n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride \n",
    "        self.padding = padding\n",
    "        self.output_shape = self.out_shape()\n",
    "        \n",
    "        # NN Layers\n",
    "        self.conv_layer = nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
    "        self.batch_norm = nn.BatchNorm2d(self.out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def refresh(self, in_channels, in_h, in_w, out_channels, kernel_size, padding = 0, stride=1):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels \n",
    "        self.kernel_size  = kernel_size \n",
    "        self.stride = stride \n",
    "        self.padding = padding\n",
    "        self.input_shape = (in_channels, in_h, in_w)\n",
    "        self.output_shape = self.out_shape()\n",
    "        \n",
    "        ## NN Layers\n",
    "        self.conv_layer = nn.Conv2d(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding)\n",
    "        self.batch_norm = nn.BatchNorm2d(self.out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "                \n",
    "    def out_shape(self):\n",
    "        c, h, w = self.input_shape\n",
    "        C = self.out_channels\n",
    "        H = (h + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        W = (w + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        return (C, H, W)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.relu(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_pool_block(nn.Module, node):  \n",
    "    def __init__(self, in_channels, in_h, in_w, kernel_size, padding = 0, stride = 1):\n",
    "        super(max_pool_node, self).__init__()    \n",
    "        node.__init__(self, (in_channels, in_h, in_w))\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride \n",
    "        self.padding = padding\n",
    "        self.output_shape = self.out_shape()\n",
    "        \n",
    "        ## NN Layer\n",
    "        self.max_pool_layer = nn.MaxPool2d(self.kernel_size, self.stride, self.padding)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.max_pool_layer(x)\n",
    "    \n",
    "    def out_shape(self):\n",
    "        c, h, w = self.input_shape\n",
    "        C = c\n",
    "        H = (h + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        W = (w + 2*self.padding - self.kernel_size)/self.stride + 1\n",
    "        return (C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class merge_block(node):\n",
    "    def __init__(self, parents, child):\n",
    "        super(merge_block, self).__init__()\n",
    "        self.describe_adj_list(parents, child)\n",
    "        \n",
    "class add_block(nn.Module, merge_block): \n",
    "    def __init__(self, parents, child):\n",
    "        super(add_block, self).__init__()\n",
    "        merge_block.__init__(self, parents, child)\n",
    "        self.input_shape = self.in_adj[0].output_shape\n",
    "        self.output_shape = self.out_shape()\n",
    "                \n",
    "    def forward(self, x, y):\n",
    "        return x+y \n",
    "        \n",
    "    def out_shape(self):\n",
    "        return self.input_shape\n",
    "\n",
    "class concat_node(merge_block):\n",
    "    # Define Later\n",
    "    pass\n",
    "\n",
    "class convex_merge_node(merge_block):\n",
    "    # Define Later\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make it a part of Net\n",
    "# class fullyConnectedBlock(nn.Module,node) :\n",
    "#     def __init__(self, net) : \n",
    "#         super(fullyConnected,self).__init__()\n",
    "#         node.__init__(self)\n",
    "#         self.net = net\n",
    "#         sh = net.int_to_node[net.topsort[-1]].output_shape\n",
    "#         out = sh[0]*sh[1]*sh[2]\n",
    "#         self.fc1 = nn.Linear(out,16)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(16,10)\n",
    "        \n",
    "#     def forward(self,x) :\n",
    "#         out = self.net(x)\n",
    "#         out  =  out.view(out.shape[0], -1)\n",
    "# #         print 'out.shape', out.shape\n",
    "# #         [1 x 75264], m2: [2352 x 16]\n",
    "#         out = self.fc1(out)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "# BATCH = 4\n",
    "# BEGIN_IN_CHANNELS = 3 \n",
    "# def addLinearLayers(net):\n",
    "# #     t = [32,3,28,28]\n",
    "#     net = fullyConnected(net)\n",
    "#     return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenBlock(nn.Module, node):\n",
    "    def __init__(self, input_shape):\n",
    "        super(FlattenBlock, self).__init__()\n",
    "        node.__init__(self, input_shape, (input_shape[0]*input_shape[1]*input_shape[2],))\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it a part of Net\n",
    "class fullyConnectedBlock(nn.Module,node) :\n",
    "    def __init__(self, input_units, output_units, activation = False) : \n",
    "        super(fullyConnectedBlock,self).__init__()\n",
    "        node.__init__(self, (input_units, ), (output_units, ))\n",
    "        self.layers = []\n",
    "        self.layers.append(nn.Linear(input_units, output_units))\n",
    "        if activation:\n",
    "            self.layers.append(nn.ReLU())\n",
    "     \n",
    "    def forward(self, x) :\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer(out)\n",
    "        return out\n",
    "\n",
    "# BATCH = 4\n",
    "# BEGIN_IN_CHANNELS = 3 \n",
    "# def addLinearLayers(net):\n",
    "# #     t = [32,3,28,28]\n",
    "#     net = fullyConnected(net)\n",
    "#     return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module, object):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.adj_mat = {}\n",
    "        self.adj_list = {}\n",
    "        self.int_to_node = {}\n",
    "        self.node_to_int = {}\n",
    "        self.nodes = []\n",
    "        self.conv_blocks = []\n",
    "        self.max_pool_blocks = [] \n",
    "        self.topsort = []\n",
    "        self.rank_in_topsort = {}\n",
    "        self.max_no = 0\n",
    "        self.node_dict = {}\n",
    "        #self.fc = nn.Linear(3 * 28 * 28, 10)\n",
    "        \n",
    "    def __init__(self, adj_list, int_to_node):\n",
    "        super(Network, self).__init__()        \n",
    "#         assert len(adj_list) == assert(int_to_node)\n",
    "        self.adj_list = adj_list\n",
    "        self.adj_mat = self.get_adj_mat(self.adj_list)\n",
    "        self.nodes = int_to_node.keys()\n",
    "        self.int_to_node = int_to_node\n",
    "        self.node_dict = {}\n",
    "        self.node_to_int = self.get_node_to_int(self.int_to_node)\n",
    "        self.max_no = max(self.int_to_node)\n",
    "        self.conv_blocks, self.max_pool_blocks = self.get_conv_and_max_pool_blocks()\n",
    "        self.topsort = []\n",
    "        self.rank_in_topsort = {}\n",
    "        self.topsorting()\n",
    "        # hardcoded part\n",
    "        #self.fc = torch.nn.Linear(3 * 28 * 28, 10)\n",
    "        \n",
    "    def get_adj_mat(self, adj_list):\n",
    "        adj_mat = {}\n",
    "        nodes = adj_list.keys()\n",
    "        for x in nodes:\n",
    "            adj_mat[x] = {}\n",
    "            for y in nodes:\n",
    "                adj_mat[x][y] = 0\n",
    "        for cnode, li in adj_list.items():\n",
    "            for par in li[0]:\n",
    "                adj_mat[par][cnode] = 1\n",
    "            for child in li[1]:\n",
    "                adj_mat[cnode][child] = 1\n",
    "        return adj_mat\n",
    "    \n",
    "    def get_node_to_int(self, int_to_node):\n",
    "        node_to_int = {}\n",
    "        for no, cnode in int_to_node.items():\n",
    "            node_to_int[cnode] = no\n",
    "        return node_to_int\n",
    "\n",
    "    def topsorting(self):\n",
    "        # level problem\n",
    "        topsort = []\n",
    "        import Queue\n",
    "        in_deg = {}\n",
    "        q = Queue.Queue()\n",
    "        for node in self.nodes:\n",
    "            val  = len(self.adj_list[node][0])\n",
    "            if val == 0:\n",
    "                q.put(node)\n",
    "            in_deg[node] = val\n",
    "            \n",
    "        while not q.empty():\n",
    "            curr_node = q.get()\n",
    "            topsort.append(curr_node)\n",
    "            for child in self.adj_list[curr_node][1]:\n",
    "                in_deg[child] -= 1\n",
    "                if in_deg[child] == 0:\n",
    "                    q.put(child)\n",
    "        self.topsort = topsort\n",
    "        self.set_rank_in_topsort()\n",
    "    \n",
    "    def set_rank_in_topsort(self):\n",
    "        for ind, node_no in enumerate(self.topsort):\n",
    "            self.rank_in_topsort[node_no]  = ind\n",
    "        \n",
    "    def createModel(self):\n",
    "         self.node_dict = torch.nn.ModuleDict(self.node_dict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.topsorting() # though this is not required here\n",
    "        outputs = {}\n",
    "        outputs[self.topsort[0]] = self.int_to_node[self.topsort[0]].forward(x)\n",
    "        for ind in range(1, len(self.topsort)):\n",
    "            node_no = self.topsort[ind]\n",
    "            curr_node = self.int_to_node[node_no]\n",
    "            outputs[node_no] = curr_node.forward(*map(lambda x: outputs[x], self.adj_list[node_no][0]))\n",
    "        return outputs[self.topsort[-1]]\n",
    "    \n",
    "    def get_conv_and_max_pool_blocks(self):\n",
    "        conv_blocks = []\n",
    "        max_pool_blocks = []\n",
    "        for x in self.nodes:\n",
    "            if isinstance(self.int_to_node[x], convolution_block):\n",
    "                conv_blocks.append(x)\n",
    "                self.node_dict[str(x)]=self.int_to_node[x]\n",
    "            elif isinstance(self.int_to_node[x], max_pool_block):\n",
    "                max_pool_blocks.append(x)\n",
    "                self.node_dict[str(x)]=self.int_to_node[x]\n",
    "        return (conv_blocks, max_pool_blocks)\n",
    "        \n",
    " \n",
    "    def add_nodes_to_network(self, nodes):\n",
    "        for curr_node in nodes:\n",
    "            if curr_node not in self.node_to_int:\n",
    "                self.max_no += 1\n",
    "                self.adj_mat[self.max_no] = {}\n",
    "                self.adj_list[self.max_no] = [[], []]\n",
    "                self.node_to_int[curr_node] = self.max_no\n",
    "                self.int_to_node[self.max_no] = curr_node\n",
    "                self.nodes.append(self.max_no)\n",
    "                if isinstance(curr_node, convolution_block):\n",
    "                    self.conv_blocks.append(self.max_no)\n",
    "                    self.node_dict[str(self.max_no)]=curr_node  \n",
    "                elif isinstance(curr_node, max_pool_block):\n",
    "                    self.max_pool_blocks.append(self.max_no)\n",
    "                    self.node_dict[str(self.max_no)]=curr_node\n",
    "                    \n",
    "        for curr_node in nodes:\n",
    "            no = self.node_to_int[curr_node]\n",
    "            self.adj_list[no] = [map(lambda x: self.node_to_int[x], curr_node.in_adj), map(lambda x: self.node_to_int[x], curr_node.out_adj)]\n",
    "#             for par in curr_node.in_adj:\n",
    "#                 self.adj_mat[self.node_to_int[par]][no] = 1\n",
    "#                 self.adj_list[self.node_to_int[par]][1].append(no)\n",
    "            for child in curr_node.out_adj:\n",
    "                self.adj_mat[no][self.node_to_int[child]] = 1\n",
    "#                 self.adj_list[self.node_to_int[child]][0].append(no)\n",
    "        self.topsorting()\n",
    "    \n",
    "    \n",
    "    def remove():\n",
    "        pass\n",
    "    \n",
    "    def deepen_morph(self):\n",
    "        deepen_conv_block = self.int_to_node[random.choice(self.conv_blocks)]\n",
    "        kernel_size = random.choice([3, 5])\n",
    "        in_channels, in_h, in_w = deepen_conv_block.output_shape\n",
    "        out_channels = in_channels\n",
    "        identity_conv_block = convolution_block(in_channels, in_h, in_w, out_channels, kernel_size, (kernel_size-1)/2)\n",
    "#         weights = identity_conv_block.conv_layer.weight.data\n",
    "        weights = identity_conv_block.conv_layer.weight\n",
    "        \n",
    "        # creating identity weights\n",
    "        for channel in range(out_channels):\n",
    "            for i in range(in_channels):\n",
    "                for j in range(kernel_size):\n",
    "                    for k in range(kernel_size):\n",
    "                        weights[channel][i][j][k] = int((channel == i) and (j == k) and j == (kernel_size)/2 )\n",
    "        \n",
    "        ## make connections \n",
    "        identity_conv_block.describe_adj_list([deepen_conv_block], deepen_conv_block.out_adj)\n",
    "        deepen_conv_block.describe_adj_list(deepen_conv_block.in_adj, [identity_conv_block])\n",
    "\n",
    "        #### later look at creating a function for singular change to in_adj or out_adj of nodes\n",
    "        for out_node in identity_conv_block.out_adj:\n",
    "            out_node_in_adj = [identity_conv_block if (x == deepen_conv_block) else x for x in out_node.in_adj ]\n",
    "            out_node.describe_adj_list(out_node_in_adj, out_node.out_adj)\n",
    "        \n",
    "        self.add_nodes_to_network([deepen_conv_block, identity_conv_block] + identity_conv_block.out_adj)\n",
    "        \n",
    "    \n",
    "    def widen_morph(self):\n",
    "        candidate_conv_blocks = []\n",
    "        for conv_block in self.conv_blocks:\n",
    "            isCandidate = bool(len(self.adj_list[conv_block][1]))\n",
    "            for child in self.adj_list[conv_block][1]:\n",
    "                isCandidate = isCandidate and isinstance(self.int_to_node[child], convolution_block)\n",
    "            if isCandidate:\n",
    "                candidate_conv_blocks.append(conv_block)\n",
    "        if len(candidate_conv_blocks) == 0:\n",
    "            return False\n",
    "\n",
    "#         parent_block_no = random.choice(candidate_conv_blocks)\n",
    "        parent_block = self.int_to_node[random.choice(candidate_conv_blocks)]\n",
    "        widening_factor = random.choice([2, 4])\n",
    "        in_channels, in_h, in_w = parent_block.input_shape\n",
    "        out_channels = parent_block.out_channels\n",
    "        kernel_size = parent_block.kernel_size\n",
    "        padding = parent_block.padding\n",
    "        stride = parent_block.stride\n",
    "#         original_parent_weight = parent_block.conv_layer.weight.data\n",
    "        original_parent_weight = parent_block.conv_layer.weight\n",
    "        #set_params\n",
    "        parent_block.refresh(in_channels, in_h, in_w, out_channels*widening_factor, kernel_size, padding, stride)\n",
    "        \n",
    "#         parent_block.conv_layer = nn.Conv2d(in_channels, out_channels*widening_factor, kernel_size, stride, padding)\n",
    "#         parent_block.batch_norm = nn.BatchNorm2d(out_channels*widening_factor)\n",
    "#         widened_parent_block = convolution_block(in_h, in_w, in_channels, out_channels*widening_factor, kernel_size, padding, stride)\n",
    "#         widened_parent_weight = parent_block.conv_layer.weight.data\n",
    "        widened_parent_weight = parent_block.conv_layer.weight\n",
    "#         print('New shape vs old shape', type(widened_parent_weight),widened_parent_weight.shape, original_parent_weight.shape)\n",
    "        widened_parent_weight[:out_channels] =torch.nn.Parameter(original_parent_weight)\n",
    "        widened_parent_weight[out_channels:] = torch.nn.Parameter(torch.zeros((out_channels*(widening_factor-1), in_channels, kernel_size, kernel_size)))\n",
    "\n",
    "#         parent_out_adj = []\n",
    "        for child in parent_block.out_adj:\n",
    "            child_no = self.node_to_int[child]\n",
    "            in_channels, in_h, in_w = child.input_shape\n",
    "            out_channels = child.out_channels\n",
    "            kernel_size = child.kernel_size \n",
    "            padding = child.padding\n",
    "            stride = child.stride\n",
    "            original_child_weight = child.conv_layer.weight\n",
    "            child.refresh(in_channels*widening_factor, in_h, in_w, out_channels, kernel_size, padding, stride)\n",
    "#             child.conv_layer = nn.Conv2d(in_channels*widening_factor, out_channels, kernel_size, stride, padding)\n",
    "#             child.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "#             child_widened = convolution_block(in_h, in_w, in_channels*widening_factor, out_channels, kernel_size, padding, stride)\n",
    "            child.conv_layer.weight[:, :in_channels, :, :] = torch.nn.Parameter(original_child_weight)\n",
    "#             child_widened.describe_adj_list([widened_parent_block if x == parent_block else x for x in child.in_adj], child.out_adj)\n",
    "#             child = child_widened\n",
    "\n",
    "#         parent_block = widened_parent_block\n",
    "        \n",
    "    ## Start from here\n",
    "    def dfs(self, curr_node, visited, weight):\n",
    "        curr_node_obj = self.int_to_node[curr_node]\n",
    "        if isinstance(curr_node_obj, FlattenBlock) or isinstance(curr_node_obj, fullyConnectedBlock):\n",
    "            return\n",
    "        visited[curr_node] = weight\n",
    "        for child in self.adj_list[curr_node][1]:\n",
    "            if child not in visited:\n",
    "                kernel = 0\n",
    "                padding = 0\n",
    "                constant = 0\n",
    "                child_node = self.int_to_node[child]\n",
    "                ## make adjustments for concatenation\n",
    "                if isinstance(child_node, convolution_block) or isinstance(child_node, max_pool_block):\n",
    "                    kernel = child_node.kernel_size\n",
    "                    padding = child_node.padding\n",
    "                    constant = 1\n",
    "                self.dfs(child, visited, [weight[0]+kernel, weight[1]+padding, weight[2]+constant])\n",
    "        \n",
    "        \n",
    "    def get_descendant_vectors(self):\n",
    "        descs = {}\n",
    "        for curr_node in self.nodes:\n",
    "            if isinstance(self.int_to_node[curr_node], FlattenBlock) or isinstance(self.int_to_node[curr_node], fullyConnectedBlock):\n",
    "                continue\n",
    "            visited = {}\n",
    "            self.dfs(curr_node, visited, [0, 0, 0])\n",
    "            del visited[curr_node] # remove root \n",
    "            descs[curr_node] = visited\n",
    "        return descs\n",
    "        \n",
    "    def skip_morph(self):\n",
    "        descs = self.get_descendant_vectors()\n",
    "        candidates = [(ans, des) for ans in descs for des in descs[ans] ]\n",
    "        no1, no2 = random.choice(candidates)\n",
    "        weight = descs[no1][no2]\n",
    "        #join outputs of node1 and node2 using a merge block\n",
    "        node_a = self.int_to_node[no1]\n",
    "        node_b = self.int_to_node[no2]\n",
    "#         print('Skip Morphism', type(node_a), type(node_b))\n",
    "        out_ch_1, out_h_1, out_w_1 = node_a.output_shape\n",
    "        out_ch_2, out_h_2, out_w_2 = node_b.output_shape\n",
    "#         print 'selected_nodes are ', no1, \"  \",no2\n",
    "#         print 'weight is   ', weight\n",
    "#         print(out_h_1, out_h_2, weight[0] - 2*weight[1] - weight[2])\n",
    "        assert(out_h_1 - out_h_2 == out_w_1 - out_w_2)\n",
    "        assert(out_h_1 - out_h_2 == weight[0] - 2*weight[1] - weight[2])\n",
    "\n",
    "        if weight[2] & 1 == 0:\n",
    "            weight[2] += 1\n",
    "            weight[0] += 1\n",
    "        weight[1] += (weight[2])/2\n",
    "        weight[2] -= 2*(weight[2]/2)\n",
    "        kernel_size = weight[0]\n",
    "        padding = weight[1]\n",
    "        stride = weight[2]\n",
    "        assert stride == 1\n",
    "        new_conv = convolution_block(out_ch_1, out_h_1, out_w_1, out_ch_2, kernel_size, padding, stride)\n",
    "        new_add = add_block([new_conv, node_b], node_b.out_adj)\n",
    "        new_conv.describe_adj_list([node_a], [new_add])\n",
    "        new_conv.conv_layer.weight = torch.nn.Parameter(torch.zeros(new_conv.conv_layer.weight.data.shape))\n",
    "        node_a.describe_adj_list(node_a.in_adj, node_a.out_adj+[new_conv])\n",
    "        for child_node in node_b.out_adj:\n",
    "            child_node.describe_adj_list([new_add if x==node_b else x for x in child_node.in_adj], child_node.out_adj)\n",
    "        node_b.describe_adj_list(node_b.in_adj, [new_add])\n",
    "        self.add_nodes_to_network([node_a, node_b, new_conv, new_add] + new_add.out_adj)\n",
    "        \n",
    "        \n",
    "    def visualize(self):\n",
    "        graph = Digraph('./assets/images/test_net', './assets/images/test_net')\n",
    "        for no, curr_node in self.int_to_node.items():\n",
    "#             graph.node(str(no), str(type(curr_node)).split('__main__.')[1])\n",
    "            graph.node(str(no), str(no) + \" :: \" + repr(curr_node))\n",
    "        for no, li in self.adj_list.items():\n",
    "            for ch in li[1]:\n",
    "                graph.edge(str(no), str(ch))\n",
    "        graph.view()\n",
    "\n",
    "    \n",
    "    def describe(self):\n",
    "        print 'Nodes: ', self.nodes\n",
    "        print 'Conv_blocks', self.conv_blocks\n",
    "        print 'Max_pool_blocks', self.max_pool_blocks\n",
    "        print 'Adj_list', self.adj_list\n",
    "        print 'Adj_mat', self.adj_mat\n",
    "        print 'int_to_node', self.int_to_node\n",
    "        print 'node_to_int', self.node_to_int\n",
    "        print 'Toposort', self.topsort\n",
    "        print 'node_dict', self.node_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 32, 32)  ->  0 -> (4, 30, 30)\n",
      "(4, 30, 30)  ->  1 -> (3, 28, 28)\n",
      "(3, 28, 28)  ->  2 -> (2352,)\n",
      "(2352,)  ->  3 -> (64,)\n",
      "(64,)  ->  4 -> (32,)\n",
      "Iteration 0 <bound method Network.widen_morph of Network()>\n",
      "Iteration 1 <bound method Network.skip_morph of Network()>\n",
      "Iteration 2 <bound method Network.skip_morph of Network()>\n",
      "Iteration 3 <bound method Network.deepen_morph of Network()>\n",
      "Iteration 4 <bound method Network.deepen_morph of Network()>\n",
      "Iteration 5 <bound method Network.deepen_morph of Network()>\n",
      "Iteration 6 <bound method Network.widen_morph of Network()>\n",
      "Iteration 7 <bound method Network.deepen_morph of Network()>\n",
      "Iteration 8 <bound method Network.widen_morph of Network()>\n",
      "Iteration 9 <bound method Network.widen_morph of Network()>\n",
      "Iteration 10 <bound method Network.skip_morph of Network()>\n",
      "Iteration 11 <bound method Network.deepen_morph of Network()>\n",
      "Iteration 12 <bound method Network.widen_morph of Network()>\n",
      "Iteration 13 <bound method Network.widen_morph of Network()>\n",
      "Iteration 14 <bound method Network.deepen_morph of Network()>\n",
      "Iteration 15 <bound method Network.skip_morph of Network()>\n",
      "Iteration 16 <bound method Network.deepen_morph of Network()>\n",
      "Iteration 17 <bound method Network.widen_morph of Network()>\n",
      "Iteration 18 <bound method Network.widen_morph of Network()>\n",
      "Iteration 19 <bound method Network.deepen_morph of Network()>\n"
     ]
    }
   ],
   "source": [
    "def test():    \n",
    "#     dummy = node((3, 32, 32), (3, 32, 32))\n",
    "    #     in_channels, in_h, in_w, out_channels, kernel_size, padding = 0, stride = 1\n",
    "    n1 = convolution_block(3, 32, 32 , 4, 3)\n",
    "    n2 = convolution_block(4, 30, 30, 3, 3)\n",
    "    flatten = FlattenBlock((3, 28, 28))\n",
    "    f1 = fullyConnectedBlock(2352, 64, True)\n",
    "    f2 = fullyConnectedBlock(64, 32)\n",
    "    \n",
    "    n1.describe_adj_list([], [n2])\n",
    "    n2.describe_adj_list([n1], [flatten])\n",
    "    flatten.describe_adj_list([n2], [f1])\n",
    "    f1.describe_adj_list([f1], [f2])\n",
    "    f2.describe_adj_list([f1], [])\n",
    "    \n",
    "\n",
    "    net = Network({0:[[], [1]], 1:[[0], [2]], 2: [[1], [3]], 3:[[2], [4]], 4:[[3], []]}, {0: n1, 1:n2, 2:flatten, 3:f1, 4:f2})\n",
    "#     net.describe() \n",
    "    net.visualize()\n",
    "    for nan in net.nodes:\n",
    "        print net.int_to_node[nan].input_shape, ' -> ', nan, '->', net.int_to_node[nan].output_shape \n",
    "    for i in range(20):\n",
    "        operations = [net.skip_morph, net.deepen_morph, net.widen_morph]\n",
    "        op = random.choice(operations)\n",
    "        print \"Iteration\", i, op\n",
    "        op()\n",
    "    net.visualize()\n",
    "#     net=addLinearLayers(net)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_net.gv     test_net.gv.pdf\r\n"
     ]
    }
   ],
   "source": [
    "! ls ./assets/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
